{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AD699 Assignment 3: Classification Trees & Random Forests\n",
    "\n",
    "**Student:** Saurabh Sharma  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load all the libraries and configure as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# OS utils\n",
    "import os\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Silence noisy warnings for cleaner output\n",
    "sns.set_style('whitegrid')  # Consistent plot aesthetics\n",
    "\n",
    "# Save the plots in this directory\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# Random seed\n",
    "SEED = 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with proper encoding; skip the second header row; use first column as index\n",
    "df = pd.read_csv('data/Colleges.csv', encoding='latin-1', index_col=0, skiprows=[1])\n",
    "\n",
    "# Basic shape check for sanity\n",
    "rows, columns = df.shape\n",
    "print(f\"Dataset shape: Rows({rows})XColumns({columns})\")\n",
    "\n",
    "# Peek at a random sample to verify loading worked\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Describe the Dataset & Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Information:\")\n",
    "print(f\"  - {len(df)} colleges\")\n",
    "print(f\"  - {len(df.columns)} variables\")\n",
    "\n",
    "print(f\"\\nVariables: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above it can be observed that there are no null columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Create Yield Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create yield = (Enroll / Accept) * 100\n",
    "df['yield'] = (df['Enroll'] / df['Accept']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yield'].sample(5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Yield Statistics:\")\n",
    "print(df['yield'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize yield distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df['yield'], bins=20, edgecolor='black')\n",
    "plt.axvline(df['yield'].median(), color='red', linestyle='--', label=f'Median = {df[\"yield\"].median():.2f}%')\n",
    "plt.xlabel('Yield (%)')\n",
    "plt.ylabel('Number of Colleges')\n",
    "plt.title('Distribution of College Yield')\n",
    "plt.legend()\n",
    "plt.savefig('outputs/yield_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yield distribution shows a roughly bell-shaped pattern with a slight right skew, ranging from approximately 10% to 100%. The median yield of 38.74% divides the dataset into equal halves for classification purposes. Most colleges cluster in the 20-50% yield range, with the peak around 30-35%, indicating typical conversion rates in higher education. The right tail extending to 100% represents highly selective institutions where nearly all accepted students enroll, while the left side represents less selective schools where students have multiple options. This distribution suggests yield is influenced by factors like selectivity, reputation, and financial aid, which our classification tree will attempt to model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the original variables\n",
    "df = df.drop(['Enroll', 'Accept'], axis=1)\n",
    "\n",
    "print(f\"New shape after dropping Enroll and Accept: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Convert Yield to Factor (High/Low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to high/low based on median\n",
    "median_yield = df['yield'].median()\n",
    "\n",
    "df['yield_category'] = df['yield'].apply(lambda x: 'high' if x >= median_yield else 'low')\n",
    "\n",
    "print(f\"Median yield: {median_yield:.2f}%\\n\")\n",
    "print(\"Class distribution:\")\n",
    "print(df['yield_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the numeric yield column\n",
    "df = df.drop('yield', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Partition Data (60/40 Train/Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "# X = Features (independent variables): All columns except the target variable\n",
    "# y = Target (dependent variable): What we're trying to predict\n",
    "X = df.drop('yield_category', axis=1)  # Remove target column to get features\n",
    "y = df['yield_category']  # Keep only the target column (high/low)\n",
    "\n",
    "# Convert Private to numeric (from categorical to binary)\n",
    "# Machine learning models need numeric input, not text\n",
    "# 'Yes' becomes 1, 'No' becomes 0\n",
    "X['Private'] = X['Private'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Fill any missing values with mean\n",
    "# Some colleges might have missing data for certain features\n",
    "# We replace NaN values with the average (mean) of that column\n",
    "# This prevents errors during model training\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nValidation set: {len(X_val)} samples\")\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Build Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the tree\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=SEED)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Tree depth: {dt_model.get_depth()}\")\n",
    "print(f\"Number of leaves: {dt_model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Display the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_model, \n",
    "          feature_names=X.columns,\n",
    "          class_names=['high', 'low'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Classification Tree for College Yield', fontsize=16)\n",
    "plt.savefig('outputs/classification_tree.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: What Did You See?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "# Create a DataFrame to store and display feature importance scores\n",
    "# Feature importance tells us which variables had the most influence on the tree's decisions\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,  # Column names from our dataset\n",
    "    'Importance': dt_model.feature_importances_  # Importance scores from the trained model (0-1 scale)\n",
    "}).sort_values('Importance', ascending=False)  # Sort from most to least important\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(importance_df.head(10))  # Display top 10 most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "The model reveals that `Room.Board` cost is the primary predictor of yield, with lower-cost colleges generally achieving higher yield. The tree's complex structure shows that predicting yield requires considering multiple factors including `tuition`, `applications`, `graduation rates`, and `student quality metrics`, as no single variable perfectly separates `high` from `low` yield colleges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Root Node Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get root node information\n",
    "# Access the internal tree structure to analyze the first split\n",
    "tree = dt_model.tree_  # Get the underlying tree structure from the trained model\n",
    "root_feature_idx = tree.feature[0]  # Index of the feature used at root (position 0 = root node)\n",
    "root_threshold = tree.threshold[0]  # The cutoff value used to split at the root\n",
    "root_feature = X.columns[root_feature_idx]  # Convert index to actual feature name\n",
    "\n",
    "print(\"Root Node Split:\")\n",
    "print(f\"  Variable: {root_feature}\")  # Which feature splits the data first\n",
    "print(f\"  Threshold: {root_threshold:.2f}\")  # At what value the split occurs\n",
    "print(f\"  Rule: If {root_feature} <= {root_threshold:.2f} go left, else go right\")  # Decision rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the root node significant?**\n",
    "\n",
    "The root node is significant because it represents the single most discriminative feature in the dataset. Room.Board at the `$3,615.50` threshold provides the maximum information gain, meaning this split best separates high-yield from low-yield colleges. As the foundation of the tree, this decision determines the initial grouping from which all subsequent splits are made. The selection of Room.Board suggests that housing affordability is a primary factor influencing student enrollment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Which Variables Appeared in the Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which features were used\n",
    "# Filter the importance dataframe to separate used and unused features\n",
    "features_used = importance_df[importance_df['Importance'] > 0]['Feature'].tolist()  # Features with importance > 0 were used in splits\n",
    "features_not_used = importance_df[importance_df['Importance'] == 0]['Feature'].tolist()  # Features with 0 importance were never selected\n",
    "\n",
    "# Display summary of feature usage\n",
    "print(f\"Features used: {len(features_used)} out of {len(X.columns)}\")\n",
    "print(f\"\\nUsed: {features_used}\")  # List of features that appear in the tree\n",
    "print(f\"\\nNot used: {features_not_used}\")  # Features ignored by the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why not all variables?**\n",
    "\n",
    "No, only 8 out of 16 features appear in the model. The decision tree algorithm selectively uses features that maximize information gain at each split. \n",
    "Variables like `F.Undergrad`, `Private`, and `Expend` were excluded because: \n",
    "\n",
    "1. They're redundant with features already used (e.g., Apps captures size), \n",
    "2. They have low predictive power for yield, \n",
    "3. The max_depth=5 constraint limits the number of splits, and \n",
    "4. The most important features (`Room.Board`, `Apps`, `Outstate`) already explain most of the variation in yield. \n",
    "\n",
    "This feature selection is actually beneficial—it creates a simpler, more interpretable model focused on the variables that truly matter for predicting enrollment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11: Confusion Matrices & Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# Use the trained decision tree to predict yield categories for both datasets\n",
    "y_train_pred = dt_model.predict(X_train)  # Predictions on training data (data the model has seen)\n",
    "y_val_pred = dt_model.predict(X_val)  # Predictions on validation data (unseen data)\n",
    "\n",
    "# Calculate accuracies\n",
    "# Compare predictions to actual values to measure performance\n",
    "train_acc = accuracy_score(y_train, y_train_pred)  # % of correct predictions on training set\n",
    "val_acc = accuracy_score(y_val, y_val_pred)  # % of correct predictions on validation set\n",
    "\n",
    "# Display results\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")  # How well model performs on training data\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")  # How well model generalizes to new data\n",
    "print(f\"Overfitting Gap: {(train_acc - val_acc):.4f}\")  # Difference indicates overfitting (larger = more overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training confusion matrix\n",
    "# A confusion matrix shows how many predictions were correct vs incorrect for each class\n",
    "print(\"Training Set:\")\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)  # Compare actual labels to predicted labels\n",
    "\n",
    "# Display in a readable DataFrame format with labels\n",
    "print(pd.DataFrame(cm_train, \n",
    "                   index=['Actual: high', 'Actual: low'],  # Rows = actual values\n",
    "                   columns=['Pred: high', 'Pred: low']))  # Columns = predicted values\n",
    "\n",
    "# Classification report provides precision, recall, F1-score for each class\n",
    "print(f\"\\n{classification_report(y_train, y_train_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation confusion matrix\n",
    "# This is the MOST IMPORTANT evaluation - shows performance on unseen data\n",
    "print(\"Validation Set:\")\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)  # Compare actual vs predicted on validation set\n",
    "# Display in DataFrame format for easy reading\n",
    "print(pd.DataFrame(cm_val,\n",
    "                   index=['Actual: high', 'Actual: low'],  # Rows = true labels\n",
    "                   columns=['Pred: high', 'Pred: low']))  # Columns = model predictions\n",
    "# Detailed metrics: precision (accuracy of positive predictions), recall (coverage), F1 (harmonic mean)\n",
    "print(f\"\\n{classification_report(y_val, y_val_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "# Create side-by-side heatmaps to compare training vs validation performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns for comparison\n",
    "\n",
    "# Left plot: Training confusion matrix\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=axes[0])  # Blue colormap, show counts\n",
    "axes[0].set_title(f'Decision Tree - Training\\nAccuracy: {train_acc:.2%}')\n",
    "axes[0].set_ylabel('Actual')  # Y-axis = true labels\n",
    "axes[0].set_xlabel('Predicted')  # X-axis = predicted labels\n",
    "\n",
    "# Right plot: Validation confusion matrix (more important!)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Oranges', ax=axes[1])  # Orange colormap for contrast\n",
    "axes[1].set_title(f'Decision Tree - Validation\\nAccuracy: {val_acc:.2%}')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between plots\n",
    "plt.savefig('outputs/decision_tree_confusion_matrices.png', dpi=300, bbox_inches='tight')  # Save to outputs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Assessment:**\n",
    "\n",
    "The decision tree achieved `78.92%` training accuracy and `69.13%` validation accuracy, showing a `9.79%` overfitting gap—acceptable but indicating some memorization of training patterns. The model performs moderately well, significantly better than random guessing (50%), but reveals a critical weakness: it's much better at identifying low-yield colleges (82% recall) than high-yield colleges (56% recall). With 68 false negatives on validation, the model is overly conservative in predicting high yield. While the ~70% validation accuracy is decent, the class imbalance in performance and moderate overfitting suggest this single decision tree has limitations that ensemble methods might address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-2: Same Dataset and Partition\n",
    "\n",
    "Using the same X_train, X_val, y_train, y_val from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Build Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest\n",
    "# n_estimators: number of trees in the ensemble\n",
    "# max_depth: limit tree depth to control overfitting\n",
    "# random_state: reproducibility\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=SEED)\n",
    "rf_model.fit(X_train, y_train)  # Train on the training split\n",
    "\n",
    "# Summarize key hyperparameters of the trained model\n",
    "print(f\"Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"Max depth per tree: {rf_model.max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "# Build a DataFrame so we can sort and inspect which features the forest relies on most\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Show the top-ranked features to see drivers of yield predictions\n",
    "print(\"Random Forest Feature Importance:\")\n",
    "print(rf_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Confusion Matrices & Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on both seen (train) and unseen (validation) data\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy metrics to check fit quality and generalization\n",
    "train_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "val_acc_rf = accuracy_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Report headline metrics and the overfitting gap (train - val)\n",
    "print(f\"Training Accuracy: {train_acc_rf:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc_rf:.4f}\")\n",
    "print(f\"Overfitting Gap: {(train_acc_rf - val_acc_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training confusion matrix helps us see class-wise correctness on data the model saw during training\n",
    "print(\"Training Set:\")\n",
    "cm_train_rf = confusion_matrix(y_train, y_train_pred_rf)\n",
    "\n",
    "# Display counts with readable row/column labels\n",
    "print(pd.DataFrame(cm_train_rf,\n",
    "                   index=['Actual: high', 'Actual: low'],\n",
    "                   columns=['Pred: high', 'Pred: low']))\n",
    "\n",
    "# Precision/recall/F1 give more detail than accuracy alone\n",
    "print(f\"\\n{classification_report(y_train, y_train_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation confusion matrix\n",
    "print(\"Validation Set:\")\n",
    "cm_val_rf = confusion_matrix(y_val, y_val_pred_rf)\n",
    "print(pd.DataFrame(cm_val_rf,\n",
    "                   index=['Actual: high', 'Actual: low'],\n",
    "                   columns=['Pred: high', 'Pred: low']))\n",
    "print(f\"\\n{classification_report(y_val, y_val_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices for train vs validation side-by-side to spot over/under-fitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training heatmap: should be high on the diagonal if the model fits training data well\n",
    "sns.heatmap(cm_train_rf, annot=True, fmt='d', cmap='Greens', ax=axes[0])\n",
    "axes[0].set_title(f'Random Forest - Training\\nAccuracy: {train_acc_rf:.2%}')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Validation heatmap: key view for generalization; compare diagonals vs off-diagonals\n",
    "sns.heatmap(cm_val_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title(f'Random Forest - Validation\\nAccuracy: {val_acc_rf:.2%}')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/random_forest_confusion_matrices.png', dpi=300, bbox_inches='tight')  # Save figure for reuse\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Assessment:**\n",
    "\n",
    "Random Forest achieved `90.11%` training accuracy and `73.95%` validation accuracy, outperforming the decision tree by `4.82` percentage points on validation data. While the model shows a larger overfitting gap (`16.15%` vs `9.79%`), the superior validation performance is what matters for real-world predictions. Critically, Random Forest dramatically improves high-yield detection—raising recall from `56%` to `72%` and achieves balanced performance across both classes (72-76% recall for both). The model reduces false negatives from 68 to 43, a 37% improvement in the most costly error type. Overall, Random Forest provides meaningfully better predictions with more balanced class performance, justifying its recommendation despite the slightly larger training-validation gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Compare the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table summarizing key metrics for both models\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Random Forest'],\n",
    "    'Training Acc': [train_acc, train_acc_rf],\n",
    "    'Validation Acc': [val_acc, val_acc_rf],\n",
    "    'Overfitting Gap': [train_acc - val_acc, train_acc_rf - val_acc_rf]\n",
    "})\n",
    "\n",
    "# Display the table without row indices for readability\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization comparing accuracy and overfitting for both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison: bars for training vs validation to check generalization\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, comparison['Training Acc'], width, label='Training', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison['Validation Acc'], width, label='Validation', alpha=0.8)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison['Model'])\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1.05])\n",
    "\n",
    "# Overfitting comparison: visualize the train-val gap for each model\n",
    "axes[1].bar(comparison['Model'], comparison['Overfitting Gap'], alpha=0.8)\n",
    "axes[1].set_ylabel('Training - Validation Accuracy')\n",
    "axes[1].set_title('Overfitting Analysis')\n",
    "axes[1].axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_comparison.png', dpi=300, bbox_inches='tight')  # Save plot for reuse\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Model Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RECOMMENDATION: Random Forest\\n\")\n",
    "\n",
    "print(\"Reasons:\")\n",
    "print(f\"1. Higher validation accuracy: {val_acc_rf:.2%} vs {val_acc:.2%}\")\n",
    "print(f\"2. Better generalization: {(train_acc_rf - val_acc_rf)*100:.1f}% gap vs {(train_acc - val_acc)*100:.1f}%\")\n",
    "print(f\"3. Ensemble approach reduces variance and improves robustness\")\n",
    "print(f\"4. More stable predictions across different data samples\")\n",
    "print(f\"\\nTrade-off: Less interpretable than single tree, but performance gain justifies this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "\n",
    "Random Forest is the superior model, achieving 73.95% validation accuracy versus Decision Tree's 69.13% a meaningful 4.82-point improvement that translates to 15 fewer errors. \n",
    "\n",
    "While Random Forest shows a larger overfitting gap (16.16% vs 9.79%), this reflects better training data fit rather than worse generalization; validation performance proves Random Forest generalizes better. \n",
    "\n",
    "Critically, Random Forest reduces false negatives from 68 to 43 (37% improvement) and achieves balanced class performance (72-76% recall) versus Decision Tree's imbalance (56-82%). The ensemble approach provides robust, stable predictions through averaging 100 trees. Unless interpretability is absolutely critical, Random Forest's superior validation accuracy and balanced performance make it the clear choice for practical yield prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metad699-assignment3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
